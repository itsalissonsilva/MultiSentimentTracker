{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "JOxXgZCJH3hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from datetime import date, timedelta\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import json\n",
        "import time\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "DLVsTmDVH5Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAoEIRbYGup9"
      },
      "outputs": [],
      "source": [
        "# Load m7 financial news dataset from (https://huggingface.co/datasets/itsalissonsilva/mag7-news-dataset)\n",
        "df = pd.read_csv('df_m7.csv', quotechar='\"', engine='python', on_bad_lines='skip')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Filter NVDA 2023\n",
        "df_nvda = df[(df['Stock_symbol'] == 'NVDA') & (df['Date'].dt.year == 2023)].copy()"
      ],
      "metadata": {
        "id": "6IYFo4HxG1yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove time (keep just the date)\n",
        "df_nvda['DateOnly'] = df_nvda['Date'].dt.date\n",
        "\n",
        "# Keep only first article per day\n",
        "df_nvda_one_per_day = df_nvda.sort_values('Date').drop_duplicates(subset='DateOnly', keep='first')"
      ],
      "metadata": {
        "id": "z-UE9MING7jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate full list of 2023 dates\n",
        "all_2023_dates = set(date(2023, 1, 1) + timedelta(days=i) for i in range(365))\n",
        "\n",
        "# Extract actual article dates\n",
        "nvda_dates = set(df_nvda_one_per_day['Date'].dt.date)\n",
        "\n",
        "# Find missing dates\n",
        "missing_nvda = sorted(all_2023_dates - nvda_dates)\n",
        "\n",
        "# Show results\n",
        "print(f\"Missing NVDA days in 2023 ({len(missing_nvda)}):\")\n",
        "print(missing_nvda)"
      ],
      "metadata": {
        "id": "QdOQZ96SHH-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download price data\n",
        "\n",
        "nvda_ticker = yf.Ticker(\"NVDA\")\n",
        "price_nvda = nvda_ticker.history(start=\"2023-01-01\", end=\"2024-01-01\")\n",
        "price_nvda = price_nvda[['Close']].reset_index()\n",
        "price_nvda['DateOnly'] = price_nvda['Date'].dt.date\n",
        "df_nvda_one_per_day['DateOnly'] = df_nvda_one_per_day['Date'].dt.date\n",
        "\n",
        "# Merge\n",
        "df_nvda_merged = pd.merge(df_nvda_one_per_day, price_nvda, on='DateOnly', how='left')\n",
        "df_nvda_merged.drop(columns=['DateOnly'], inplace=True)"
      ],
      "metadata": {
        "id": "1GqhyDeqHOjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only articles on trading days\n",
        "\n",
        "valid_nvda_dates = set(price_nvda['DateOnly'])\n",
        "df_nvda_trading_days = df_nvda_one_per_day[df_nvda_one_per_day['DateOnly'].isin(valid_nvda_dates)].copy()\n",
        "\n",
        "# Merge with price data from before\n",
        "df_nvda_merged = pd.merge(df_nvda_trading_days, price_nvda[['DateOnly', 'Close']], on='DateOnly', how='left')\n",
        "\n",
        "df_nvda_merged.drop(columns=['DateOnly'], inplace=True)"
      ],
      "metadata": {
        "id": "C34pcrTqHVmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key='YOURAPIKEY')"
      ],
      "metadata": {
        "id": "O91f63YrHgiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment(summary, model=\"gpt-4\"):\n",
        "    prompt = f\"\"\"\n",
        "You are a financial sentiment analyst.\n",
        "\n",
        "Given the following news summary about a stock, assign a score from 0 to 1 (rounded to 2 decimal places) for each of the following sentiment dimensions:\n",
        "\n",
        "- Optimism (positive forward-looking sentiment)\n",
        "- Uncertainty (vagueness, ambiguity, or risk)\n",
        "- Surprise (unexpected developments)\n",
        "- Immediacy (urgency or time sensitivity)\n",
        "- Relief (easing of prior concerns)\n",
        "\n",
        "Only respond in valid JSON format.\n",
        "\n",
        "Summary:\n",
        "\\\"\\\"\\\"{summary}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0,\n",
        "        )\n",
        "        content = response.choices[0].message.content\n",
        "        return json.loads(content)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return {\n",
        "            \"Optimism\": None,\n",
        "            \"Uncertainty\": None,\n",
        "            \"Surprise\": None,\n",
        "            \"Immediacy\": None,\n",
        "            \"Relief\": None\n",
        "        }\n"
      ],
      "metadata": {
        "id": "iKqwEeCyHmH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use tqdm to monitor progress\n",
        "summaries = df_nvda_merged['Lsa_summary'].fillna(\"\").tolist()\n",
        "scores = []\n",
        "\n",
        "for summary in tqdm(summaries, desc=\"Scoring sentiment\"):\n",
        "    if not summary.strip():\n",
        "        scores.append({\n",
        "            \"Optimism\": None,\n",
        "            \"Uncertainty\": None,\n",
        "            \"Surprise\": None,\n",
        "            \"Immediacy\": None,\n",
        "            \"Relief\": None\n",
        "        })\n",
        "        continue\n",
        "    try:\n",
        "        result = analyze_sentiment(summary)\n",
        "    except:\n",
        "        result = {\n",
        "            \"Optimism\": None,\n",
        "            \"Uncertainty\": None,\n",
        "            \"Surprise\": None,\n",
        "            \"Immediacy\": None,\n",
        "            \"Relief\": None\n",
        "        }\n",
        "    scores.append(result)\n",
        "    time.sleep(1)  # adjust if needed"
      ],
      "metadata": {
        "id": "t3bDPHfEHqya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_nvda_final['Date'] = pd.to_datetime(df_nvda_final['Date'])\n",
        "\n",
        "print(\"Date range:\", df_nvda_final['Date'].min().date(), \"to\", df_nvda_final['Date'].max().date())\n",
        "\n",
        "print(\"Unique days with articles:\", df_nvda_final['Date'].dt.date.nunique())\n",
        "\n",
        "# Check for missing close prices\n",
        "missing_prices = df_nvda_final['Close'].isna().sum()\n",
        "print(\"Missing 'Close' prices:\", missing_prices)"
      ],
      "metadata": {
        "id": "CJXNG4ROHwh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_nvda_final['Date'] = pd.to_datetime(df_nvda_final['Date'])\n",
        "\n",
        "start_date = df_nvda_final['Date'].min().strftime('%Y-%m-%d')\n",
        "end_date = df_nvda_final['Date'].max().strftime('%Y-%m-%d')\n",
        "\n",
        "filename = f\"nvda_final_{start_date}_to_{end_date}.csv\"\n",
        "\n",
        "df_nvda_final.to_csv(filename, index=False)"
      ],
      "metadata": {
        "id": "pU1S2c8yHycJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}